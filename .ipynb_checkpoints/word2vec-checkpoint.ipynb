{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicekwon/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/Users/alicekwon/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Dungeons &amp;amp; Dragons Player's Handbook (Core...</td>\n",
       "      <td>Wizards RPG Team</td>\n",
       "      <td>Create heroic characters for the world’s great...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tasha's Cauldron of Everything (D&amp;amp;D Rules ...</td>\n",
       "      <td>Wizards RPG Team</td>\n",
       "      <td>A magical mixture of rules options for the wor...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Card Night: Classic Games, Classic Decks, and ...</td>\n",
       "      <td>Will Roya</td>\n",
       "      <td>Learn when to hold 'em and when to fold 'em wi...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The Queen's Gambit (Television Tie-in)</td>\n",
       "      <td>Walter Tevis</td>\n",
       "      <td>Engaging and fast-paced, this gripping coming-...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Answer Is...: Reflections on My Life</td>\n",
       "      <td>Alex Trebek</td>\n",
       "      <td>A RECOMMENDED SUMMER READ BY THE NEW YORK TIME...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title            author  \\\n",
       "0  Dungeons &amp; Dragons Player's Handbook (Core...  Wizards RPG Team   \n",
       "1  Tasha's Cauldron of Everything (D&amp;D Rules ...  Wizards RPG Team   \n",
       "2  Card Night: Classic Games, Classic Decks, and ...         Will Roya   \n",
       "3             The Queen's Gambit (Television Tie-in)      Walter Tevis   \n",
       "4           The Answer Is...: Reflections on My Life       Alex Trebek   \n",
       "\n",
       "                                            overview                  genre  \n",
       "0  Create heroic characters for the world’s great...  Activity & Game Books  \n",
       "1  A magical mixture of rules options for the wor...  Activity & Game Books  \n",
       "2  Learn when to hold 'em and when to fold 'em wi...  Activity & Game Books  \n",
       "3  Engaging and fast-paced, this gripping coming-...  Activity & Game Books  \n",
       "4  A RECOMMENDED SUMMER READ BY THE NEW YORK TIME...  Activity & Game Books  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('book_dataframe.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of genres\n",
    "genres = df['genre'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group dataframe by genre\n",
    "grouped_by_genre =  df.groupby(df.genre)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary with keys = genre and values = df groupedby that genre\n",
    "genre_dict = dict.fromkeys(genres, 0)\n",
    "for genre in genres:\n",
    "    genre_dict[genre] = grouped_by_genre.get_group(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that our train_df takes randomly 80% and not random 80% of the entire original dataframe so in our training set will contain the same amount of examples from each genre.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [dataframes.sample(frac = 0.8) for dataframes in genre_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixes some indexing issue which hopefully can be resolved more cleverly\n",
    "train_df = train_df.reset_index()\n",
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('index', axis = 1, inplace = True)\n",
    "test_df.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use doc2vec for prediction of genre using titles of the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title_df = train_df[['title', 'genre']]\n",
    "test_title_df = test_df[['title', 'genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Answer Is...: Reflections on My Life</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Secret: A Treasure Hunt</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>New York Times Light and Easy Crossword Puzzles</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Dungeons &amp;amp; Dragons Dungeon Master's Guide ...</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Anatomy Coloring Book</td>\n",
       "      <td>Activity &amp; Game Books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  genre\n",
       "0           The Answer Is...: Reflections on My Life  Activity & Game Books\n",
       "1                        The Secret: A Treasure Hunt  Activity & Game Books\n",
       "2    New York Times Light and Easy Crossword Puzzles  Activity & Game Books\n",
       "3  Dungeons &amp; Dragons Dungeon Master's Guide ...  Activity & Game Books\n",
       "4                          The Anatomy Coloring Book  Activity & Game Books"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to remove punctuations and symbols etc., (maybe theres a better way than this?)\n",
    "def clean_text(text): \n",
    "    text = re.sub(r'&', r' ', text)\n",
    "    text = re.sub(r'#', r' ', text)\n",
    "    text = re.sub(r';', r' ', text)\n",
    "    text = re.sub(r':', r' ', text)\n",
    "    text = re.sub(r'!', r' ', text)\n",
    "    text = re.sub(r'\\d', r' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace(')', '')\n",
    "    return text\n",
    "\n",
    "#define a function to remove stop-words and tokenize text using NLTK tokenizer.\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    \n",
    "    tokens_without_sw = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return tokens_without_sw\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title_df['title'] = train_title_df['title'].apply(lambda r: clean_text(r))\n",
    "test_title_df['title'] = test_title_df['title'].apply(lambda r: clean_text(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, one should have done this before splitting to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_title = train_title_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['title']), tags=[r.genre]), axis=1)\n",
    "test_title = test_title_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['title']), tags=[r.genre]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['violence', 'speed', 'momentum'], tags=['Activity & Game Books']),\n",
       "       TaggedDocument(words=['candlekeep', 'mysteries', 'amp', 'adventure', 'book', 'dungeons', 'amp', 'dragons'], tags=['Activity & Game Books']),\n",
       "       TaggedDocument(words=['games', 'puzzles', 'amp', 'trivia', 'challenges', 'specially', 'designed', 'keep', 'brain', 'young'], tags=['Activity & Game Books']),\n",
       "       ...,\n",
       "       TaggedDocument(words=['humans', 'new', 'york', 'stories'], tags=['Travel']),\n",
       "       TaggedDocument(words=['humans', 'amp', 'exclusive', 'edition'], tags=['Travel']),\n",
       "       TaggedDocument(words=['abu', 'dhabi', 'bar', 'mitzvah', 'fear', 'love', 'modern', 'middle', 'east'], tags=['Travel'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:00<00:00, 806239.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=0, vector_size=30, window = 10, min_count=1, sample = 1e5, workers=cores)\n",
    "model.build_vocab([x for x in tqdm(train_title.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:00<00:00, 535686.76it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 795798.67it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 574419.83it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 830122.67it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 876787.63it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 868575.22it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 894157.37it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 796420.00it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 672948.10it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 337119.02it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 511356.89it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 308714.59it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 840519.72it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 666615.30it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1009755.23it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 524341.90it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 267436.09it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 442194.70it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 929688.97it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 538060.31it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 448770.23it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 889014.06it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 809567.25it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 866803.82it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 178225.31it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 452071.77it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 856325.33it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 263525.56it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 353792.57it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 731222.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 104 ms, total: 1.79 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model.train([x for x in tqdm(train_title.values)], total_examples=len(train_title.values), epochs=1)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which better 'converges' vectors for tagged words for each tag\n",
    "def learning_vec(model, tagged_docs):\n",
    "    sents = tagged_docs.values \n",
    "    genre, arr_of_words = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=100)) for doc in sents])\n",
    "    return genre, arr_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = learning_vec(model, train_title)\n",
    "y_test, X_test = learning_vec(model, test_title)\n",
    "logreg = LogisticRegression(solver='liblinear', C=1e5)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.08552631578947369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summ_df = train_df[['overview', 'genre']]\n",
    "test_summ_df = test_df[['overview', 'genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summ_df['overview'] = train_summ_df['overview'].apply(lambda r: clean_text(r))\n",
    "test_summ_df['overview'] = test_summ_df['overview'].apply(lambda r: clean_text(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summ = train_summ_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['overview']), tags=[r.genre]), axis=1)\n",
    "test_summ = test_summ_df.apply(lambda r: TaggedDocument(words=tokenize_text(r['overview']), tags=[r.genre]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:00<00:00, 733957.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(dm=0, vector_size=30, negative=5, window = 5, hs=0, min_count=1, sample = 1e5, workers=cores)\n",
    "model.build_vocab([x for x in tqdm(train_summ.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:00<00:00, 1141767.11it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 567959.21it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1419107.86it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 931897.25it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1239434.67it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1381439.24it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 844975.76it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1657008.99it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1084703.03it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1833971.11it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1209742.33it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1722483.51it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1299101.80it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1352498.98it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 714424.10it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1739520.35it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 958337.78it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1670577.68it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1025180.64it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 839827.71it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1562583.84it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1149486.97it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1346784.70it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1449353.13it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1342177.28it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1463073.34it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1702929.44it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1696697.83it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1425453.79it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 1548352.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 501 ms, total: 12.6 s\n",
      "Wall time: 7.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model.train(utils.shuffle([x for x in tqdm(train_summ.values)]), total_examples=len(train_summ.values), epochs=1)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = learning_vec(model, train_summ)\n",
    "y_test, X_test = learning_vec(model, test_summ)\n",
    "logreg = LogisticRegression(solver='liblinear', C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.1118421052631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
